<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator>
  <link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2019-02-10T20:08:24-05:00</updated>
  <id>http://localhost:4000/</id>

  
    <title type="html">Meehl Technologies</title>
  

  

  

  
  
    <entry>
      <title type="html">Building a tighter analysis and prototyping workflow</title>
      <link href="http://localhost:4000/2019/01/25/building-tighter-computational-analysis-work-flow/" rel="alternate" type="text/html" title="Building a tighter analysis and prototyping workflow" />
      <published>2019-01-25T00:00:00-05:00</published>
      <updated>2019-01-25T00:00:00-05:00</updated>
      <id>http://localhost:4000/2019/01/25/building-tighter-computational-analysis-work-flow</id>
      <content type="html" xml:base="http://localhost:4000/2019/01/25/building-tighter-computational-analysis-work-flow/">&lt;p&gt;Human powered flight has historically been a particularly hard engineering nut to crack. An engineering challenge set by Brittish industrialist Henry Kremer in 1959 went unmet for 18 years — fly an entirely human powered aircraft in a figure eight pattern around two poles a half-mile apart. For almost 2 decades, the £50,000 prize remained out of reach, as dozens of teams failed to achieve the feat. Finally, a man named Paul MacCready and his team were able to meet the challenge within just six months of attempting it. In 1977, the MacCready &lt;a href=&quot;https://en.wikipedia.org/wiki/MacCready_Gossamer_Condor&quot;&gt;Gossamer Condor&lt;/a&gt; became the first human powered aircraft capable of controlled and sustained flight. Just two years later, MacCready and team won the second &lt;a href=&quot;https://en.wikipedia.org/wiki/Kremer_prize&quot;&gt;Kremer prize&lt;/a&gt; of £100,000 when the &lt;a href=&quot;https://en.wikipedia.org/wiki/MacCready_Gossamer_Albatross&quot;&gt;Gossamer Albatross&lt;/a&gt; flew from England to France. What was so different about this team? What enabled them to find success where others could not?&lt;/p&gt;

&lt;h3 id=&quot;a-change-in-focus&quot;&gt;a change in focus&lt;/h3&gt;

&lt;p&gt;The secret to their success was to recognize that the problem that needed to be solved was not that of engineering human powered flight, but that the prototyping process for designing the aircraft was inefficient. MacCready recognized that the prototype-test-redesign loop was far too inefficient. While other teams were taking much longer – a year or more – to build their prototype aircraft, the MacCready team focused on finding ways to build, modify, and rebuild the aircraft quickly. Ultimately, the MacCready team was able to turn out a new prototype sometimes twice a day, while other teams took much, much longer to build their prototypes. It was this simple change in focus – from engineering the aircraft, to engineering the process – that enabled the team to vastly improve their prototyping efficiency, and ultimately, their engineering velocity.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We often see a similar pattern in our own work. Perhaps it’s human nature to attack a problem or pursue a goal directly – almost blindly. We all want to feel that we’re making progress toward a goal; and attacking a problem head-on can often make us feel productive. This can lead us into work patterns that are less than efficient, perhaps without our awareness. So it’s often beneficial to step out of the heads-down work mode to examine how we’re solving problems.&lt;/p&gt;

&lt;div class=&quot;lb-image &quot;&gt;
  &lt;a href=&quot;/img/building-tighter-analysis/dev_path.png&quot; title=&quot;A shorter path to success&quot; data-lightbox=&quot;/img/building-tighter-analysis/dev_path.png&quot; data-title=&quot;A shorter path to success&quot;&gt;
    &lt;div&gt;&lt;img src=&quot;/img/building-tighter-analysis/dev_path.png&quot; /&gt;&lt;/div&gt;
  &lt;span class=&quot;caption&quot;&gt;A shorter path to success&lt;/span&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;Often, bringing a new product to market involves research and development encompassing an iterative workflow. Whether the product is hardware, software, pharmaceutical or something else, there’s a build/test/analyze/modify loop that plays a huge roll in how fast we’re able to bring the product to market, and at what cost. Looking for ways to eliminate inefficiencies in that workflow loop can lead to huge gains in development velocity and cost, and a shorter path to success. It can be the difference between project success and failure; going to market and being beat to market; finishing in budget and blowing the budget. One thing that I like about the software world is that it offers us lots of opportunities to address such inefficiencies.&lt;/p&gt;

&lt;h3 id=&quot;hacking-the-workflow&quot;&gt;hacking the workflow&lt;/h3&gt;
&lt;p&gt;Not long ago, I worked with a client (let’s call them X Co) who is using computational analysis to design, test, and build their hardware product (XWare). In the process of designing and testing XWare, they had to endure a very cumbersome workflow loop involving quite a bit of manual work and a lot of repetition. This workflow consumed an enormous amount of time,  physical and mental energy, and caused X Co to struggle to meet milestones.&lt;/p&gt;

&lt;p&gt;The X Co workflow involved doing parametric studies to search for heuristic success indicators within a data set produced using computational flow analysis algorithms. That’s a bit  of a long-winded way of saying they need to repeatedly  and iteratively generate data in order to search through and analyze the data, looking for indicators of a successful design. At a high level, the process looks like this:&lt;/p&gt;

&lt;div class=&quot;lb-image &quot;&gt;
  &lt;a href=&quot;/img/building-tighter-analysis/old_study_process.png&quot; title=&quot;The original workflow&quot; data-lightbox=&quot;/img/building-tighter-analysis/old_study_process.png&quot; data-title=&quot;The original workflow&quot;&gt;
    &lt;div&gt;&lt;img src=&quot;/img/building-tighter-analysis/old_study_process.png&quot; /&gt;&lt;/div&gt;
  &lt;span class=&quot;caption&quot;&gt;XWare parametric study&lt;/span&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;Generate computational flow analysis data for a specific combination of XWare physical and environmental parameters. Each specific combination of parameters, known as a case, represents specific features of XWare and the environment it operates in. The computational flow algorithms analyze these features in order to generate specific performance data for a version of XWare.&lt;/li&gt;
  &lt;li&gt;Manually search the data set associated with the case for heuristic success indicators in order to accept or eliminate the solution represented by that case.&lt;/li&gt;
  &lt;li&gt;Manually perform analysis and produce and annotate graphs and other media for a successful case, or reject the unsuccessful case.&lt;/li&gt;
  &lt;li&gt;Repeat the above steps for every combination of parameters, in order to analyze every possible case in the study. This produces a large corpus of data and analysis artifacts which need to be collected, sorted, aggregated, and distilled into a final analysis.&lt;/li&gt;
  &lt;li&gt;Finally, aggregate analysis and results from each successful case in order to produce a full parametric study.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The process of producing the computational flow data involved hand modifying a parameter and manually producing a data set for a new case. This produced a large, relatively disorganized data set representing a single case, which needed to be stored in an organized fashion for analysis. In addition, a practitioner would be required to wait quite some time while the data was being generated for the case. After data generation completed, he could begin to manually search the data set for heuristic success indicators, using a spreadsheet to look at a sea of numbers from raw CSV data. After this tedious process, the task of building and hand annotating graphs and other analysis media would begin. Finally, all of this work needed to be repeated for every combination of parameters in the search space.&lt;/p&gt;

&lt;p&gt;Having to repeat the entire cumbersome manual process for every combination of parameters is a Real Problem. To see why, consider that the number of cases to be analyzed grows multiplicatively with the number of possible values for each parameter in the study. So adding even a single new parameter value to the search space could greatly expand the size of the search space, leading to an enormous amount of new work.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;redefine-the-problem-from-how-can-we-build-a-great-xware-to-how-can-we-build-a-great-xware-design-process&quot;&gt;redefine the problem from “how can we build a great XWare?” to “how can we build a great XWare design process?”&lt;/h2&gt;

&lt;p&gt;Working with X Co, we were able to understand their workflow, and ultimately redefine the problem from “how can we build a great XWare?” to “how can we build a great XWare design process?”. We focused on building an efficient XWare design process that also empowers us to continuously improve the process itself. We chose technologies and made design decisions that we believed would lead to a system that is capable of growing and evolving. We started by defining three goals for the project:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;eliminate the cumbersome workflow loop X Co found themselves working in&lt;/li&gt;
  &lt;li&gt;simplify and refine the process of doing analysis on the data&lt;/li&gt;
  &lt;li&gt;build a system that enables continuous process evolution, to enable growth that keeps pace with business needs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;building-for-abstraction&quot;&gt;building for abstraction&lt;/h3&gt;
&lt;p&gt;Our first task was to build a system that automates data generation, combining all data from all cases into a single search space. The new system allows X Co to generate all of the data in a search space by defining parameter values up front and just letting it run. This abstraction greatly compresses the amount of time required to generate test data, and frees up valuable brain real-estate to focus on analysis.&lt;/p&gt;

&lt;p&gt;We chose to employ &lt;a href=&quot;http://lucene.apache.org/solr/&quot;&gt;Apache Solr&lt;/a&gt; as our data store. No doubt, some of you are asking, “why the hell would you use a search engine as a database for computational data?”. Well, when you think about it, Solr is actually a database — but with a more powerful query language than a typical SQL database. It’s fast, scalable, easy to deploy and configure, and capable of handling a wide range of use cases. Most of all, the query language lends itself well to teasing insights out of data. It’s often impossible to know what you’ll want to ask of your data in the future. So it’s important to choose a data store and query paradigm that can evolve with your needs, as they arise. We often reach for Solr in our data architecture projects.&lt;/p&gt;

&lt;p&gt;We chose to use Venturi to aggregate and normalize the data, and move it from flat files to Solr as it’s generated. Venturi is a framework we built and use internally with our clients to build scalable data processing (ETL) engines. It employs a pluggable architecture and a custom transformation description language that makes it easy to describe a transformation pipeline, and route data from a data source to a data sink. Venturi also greatly simplifies the process of updating and iterating on the transformation pipeline to address changing data or business requirements. And since it’s scalable out of the box, it can grow with the size of the data set. So our use of Venturi builds nicely on our goal of enabling continuous process evolution.&lt;/p&gt;

&lt;h3 id=&quot;refining-the-analysis&quot;&gt;refining the analysis&lt;/h3&gt;
&lt;p&gt;Having addressed data generation, we turned to data retrieval and analysis, aiming to simplify and automate heuristic search where it made sense.
There were a number of concerns for this part of the project, including the size of the data set (and the high probability it will continue to grow), convenient remote access, and support for multiple users. So a web based analysis portal made sense for this project.
In addition to supporting scalability and enabling portability, it allows for fast deployments of fixes and enhancements.&lt;/p&gt;

&lt;p&gt;Working with X Co, we defined a standard set of heuristic visualizations they use to analyze each case.
The portal was designed to generate these visualizations automatically for the full study.
Pulling all of the data for a study into a single search space enables analysis at a level previously unavailable to X Co.
Starting with the standard visualizations, users can use filters to limit parameter values, eliminating irrelevant cases and updating their visualizations in real-time.
Using the new portal, it’s easy to quickly visualize success cases, determine successful parametric ranges, and compare multiple cases with one another.
The portal also makes it easy to visualize and eliminate failure cases from the study, so that focus can remain on the success cases.
Finally, the portal makes it easy to export analysis graphics along with the underlying data, in order to build the full parametric study.&lt;/p&gt;

&lt;h3 id=&quot;summing-up&quot;&gt;summing up&lt;/h3&gt;
&lt;p&gt;By isolating repetition and encapsulating it into a software abstraction, X Co realized a surprisingly large gain in development velocity.
And they have the ability to continue improving their development process over time, thanks to a software system designed from the ground up to be malleable.
With continued diligence, their development process will grow and evolve with them.
Most importantly, their gains will continue to compound, as they have more time and mental real-estate to focus on the other aspects of their path to success.&lt;/p&gt;

&lt;p&gt;The inefficient iterative workflow loop we saw with X Co is surprisingly common. Unfortunately, it can be difficult to step out of the work to examine how it can be done more efficiently.
But doing just that can be an important part of finding success – and it can lead to big, relatively cheap gains in development velocity.
Any time we observe ourselves doing repetitive work, it could be regarded as the canary in the coal mine, and an opportunity to improve our approach.
Can you identify the ways that you’re working inefficiently?
Next time you do, think about how can you improve your approach to your work and increase your own velocity.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>keith</name>
        
        
      </author>

      

      
        <category term="ETL," />
      
        <category term="computational" />
      
        <category term="analysis," />
      
        <category term="venturi" />
      

      
        <summary type="html">Human powered flight has historically been a particularly hard engineering nut to crack. An engineering challenge set by Brittish industrialist Henry Kremer in 1959 went unmet for 18 years — fly an entirely human powered aircraft in a figure eight pattern around two poles a half-mile apart. For almost 2 decades, the £50,000 prize remained out of reach, as dozens of teams failed to achieve the feat. Finally, a man named Paul MacCready and his team were able to meet the challenge within just six months of attempting it. In 1977, the MacCready Gossamer Condor became the first human powered aircraft capable of controlled and sustained flight. Just two years later, MacCready and team won the second Kremer prize of £100,000 when the Gossamer Albatross flew from England to France. What was so different about this team? What enabled them to find success where others could not?</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Working with the Lucene / Solr TokenStream API</title>
      <link href="http://localhost:4000/2019/01/22/token-streams/" rel="alternate" type="text/html" title="Working with the Lucene / Solr TokenStream API" />
      <published>2019-01-22T00:00:00-05:00</published>
      <updated>2019-01-22T00:00:00-05:00</updated>
      <id>http://localhost:4000/2019/01/22/token-streams</id>
      <content type="html" xml:base="http://localhost:4000/2019/01/22/token-streams/">&lt;p&gt;I was recently tasked with a project for a client involving Lucene / Solr
tokenizers and token streams. It’s common for most Solr users to only have to
deal with tokenizers at a high level, usually when they’re building out
their schema and deciding how their searchable content needs to be broken up
and filtered. For example, adding a standard or whitespace tokenizer, a synonym filter, and a stemmer to the schema. However, the details of this particular project required a deeper level
of involvement. During my investigation on how these things work at a lower
level, I found there was very little documentation available. In fact, the only
documentation I was able to find aside from the javadocs was a blog post by &lt;a href=&quot;http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html&quot;&gt;Michael McCandless&lt;/a&gt;
that is almost seven years old! For those people wanting to use the API at a lower level, perhaps to write your own TokenFilter or do some custom tokenization, this isn’t much to go on, so I thought I’d do a bit of an update as well as write about some stumbling blocks I came across. But first, let’s do a quick API introduction.&lt;/p&gt;

&lt;h3 id=&quot;tokenstreams-and-tokenizers&quot;&gt;TokenStreams and Tokenizers&lt;/h3&gt;
&lt;p&gt;TokenStream is the base class that represents a string of text that is intended to either be indexed
or used as a query. Specifically, a TokenStream represents the enumeration of
the &lt;em&gt;tokens&lt;/em&gt; of the string of text. That is, it represents the components of
the text after it’s been broken into suitable pieces based on your use case (tokenized).
For simple written human text, this typically means splitting on whitespace
and punctuation, and typically ‘throwing away’ punctuation in a way that
allows the user (the person writing the search query) to not have to type and
match the exact text. You can think of tokenization as a way to normalize and filter both what’s being stored in the index and the search query in a way that enables matching from query to index. For example, imagine one of the documents in the index
contains the sentence:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;It’s a sure fire formula for success.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If this sentence were indexed without tokenization, then a user searching for “sure-fire” would not find this document, because that text does not strictly match the indexed sentence.
That’s basic tokenization. So what part does a TokenStream &lt;em&gt;actually&lt;/em&gt; play? Well, as Mike McCandless explained in his blog post, you can think of TokenStreams in Lucene as a graph where each node is a position and each arc is a token.&lt;/p&gt;

&lt;div class=&quot;lb-image &quot;&gt;
  &lt;a href=&quot;/img/token-streams/token-stream2.png&quot; title=&quot;TokenStream as a graph&quot; data-lightbox=&quot;/img/token-streams/token-stream2.png&quot; data-title=&quot;TokenStream as a graph&quot;&gt;
    &lt;div&gt;&lt;img src=&quot;/img/token-streams/token-stream2.png&quot; /&gt;&lt;/div&gt;
  &lt;span class=&quot;caption&quot;&gt;A white-space tokenized string with synonyms&lt;/span&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;The underlying structure of the code isn’t always a graph in the sense that you’ve got one node pointing to another like you’d traditionally think of a graph, but it can be helpful to think about it that way. TokenStream has Attributes that store all of the information you’re interested in about each token. When a TokenStream is to be consumed, &lt;code class=&quot;highlighter-rouge&quot;&gt;incrementToken()&lt;/code&gt; is called repeatedly as part of the TokenStream lifecycle. TokenStram’s job is then to setup the attributes for the token it’s currently iterating over. For example, one of the goals of the system is to be able to keep track of positions in the original text while manipulating the underlying representation. A good example is what happens when you want to search using synonyms. In the example sentence above, we added ‘sure-fire’, ‘method’, and ‘winning’ as synonyms. When a user searches for ‘sure fire method’, they still find the document and the hit highlighting still properly highlights ‘sure fire formula’ even though the phrases are different lengths.&lt;/p&gt;

&lt;div class=&quot;lb-image &quot;&gt;
  &lt;a href=&quot;/img/token-streams/tokenstream-hierarchy.png&quot; title=&quot;TokenStream class diagram&quot; data-lightbox=&quot;/img/token-streams/tokenstream-hierarchy.png&quot; data-title=&quot;TokenStream class diagram&quot;&gt;
    &lt;div&gt;&lt;img src=&quot;/img/token-streams/tokenstream-hierarchy.png&quot; /&gt;&lt;/div&gt;
  &lt;span class=&quot;caption&quot;&gt;TokenStream class diagram&lt;/span&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;There are basically two types of TokenStreams: Tokenizer and TokenFilter.&lt;br /&gt;
A Tokenizer is a TokenStream that takes a Reader as input. Tokenizers are used to transform an input text into a TokenStream through a Reader. This mainly happens as the first step to indexing a piece of content. In other words, the analyzer (starting point) will initially create a TokenStream using a Tokenizer and the following steps will transform it with TokenFilters.&lt;br /&gt;
TokenFilter takes an existing TokenStream and manipulates or filters it in some way. There are lots of TokenFilters already implemented in the Lucene API. Synonyms in Lucene-Solr are implemented as a TokenFilter, essentially adding new arcs to the graph for each synonym.&lt;/p&gt;

&lt;h3 id=&quot;the-tokenstream-lifecycle&quot;&gt;The TokenStream Lifecycle&lt;/h3&gt;
&lt;p&gt;When TokenStreams are consumed, there’s a lifecycle they follow: &lt;code class=&quot;highlighter-rouge&quot;&gt;reset()&lt;/code&gt;  → &lt;code class=&quot;highlighter-rouge&quot;&gt;incrementToken()&lt;/code&gt; → &lt;code class=&quot;highlighter-rouge&quot;&gt;end()&lt;/code&gt; → &lt;code class=&quot;highlighter-rouge&quot;&gt;close()&lt;/code&gt;. The &lt;code class=&quot;highlighter-rouge&quot;&gt;incrementToken()&lt;/code&gt; method is where the consumer actually consumes one token at a time, repeatedly, until there are no more tokens to consume. But first, the consumer must &lt;code class=&quot;highlighter-rouge&quot;&gt;reset()&lt;/code&gt; the input TokenStream.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;  &lt;span class=&quot;cm&quot;&gt;/**
   * This method is called by a consumer before it begins consumption using
   * {@link #incrementToken()}.
   * &amp;lt;p&amp;gt;
   * Resets this stream to a clean state. Stateful implementations must implement
   * this method so that they can be reused, just as if they had been created fresh.
   * &amp;lt;p&amp;gt;
   * If you override this method, always call {@code super.reset()}, otherwise
   * some internal state will not be correctly reset (e.g., {@link Tokenizer} will
   * throw {@link IllegalStateException} on further usage).
   */&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;throws&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IOException&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;Tokenizer.java - &lt;a href=&quot;https://github.com/apache/lucene-solr/blob/branch_7_6/lucene/core/src/java/org/apache/lucene/analysis/TokenStream.java&quot;&gt;Solr
7.6 Github&lt;/a&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Resets this stream to a clean state. Stateful implementations must implement this method so that they can be reused, just as if they had been created fresh&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is the important bit of that code snippet. This allows you to consume some number of tokens and then start at the beginning again for further processing. It also allows further steps in the chain to reprocess a TokenStream that’s already been processed. After consuming all tokens, the consumer will &lt;code class=&quot;highlighter-rouge&quot;&gt;end()&lt;/code&gt; and then &lt;code class=&quot;highlighter-rouge&quot;&gt;close()&lt;/code&gt; the TokenStream. It is then ready to move on to the next step where additional consumers may repeat the process on the newly created stream.&lt;/p&gt;

&lt;h3 id=&quot;beware-the-tokenizer-lifecycle&quot;&gt;Beware the Tokenizer Lifecycle!&lt;/h3&gt;
&lt;p&gt;Here’s our first pitfall. In the project I was working on, I was using the relatively new ConcatenatingTokenStream class to assemble a few streams together. I initially tried creating a new stream using an existing Tokenizer class and then feeding that into ConcatenatingTokenStream, but I couldn’t make it work. I kept running into IllegalStateException telling me that I was violating the TokenStream (lifecycle) contract. After a bit of digging, I found out what was going on. It turns out that Tokenizer implementations themselves violate the contract. In fact, as it stands today, Tokenizer implementations can go through their lifecycle exactly once. No Tokenizer implementation can be &lt;code class=&quot;highlighter-rouge&quot;&gt;reset()&lt;/code&gt; more than one time. The situation ended up this way because currently Tokenizers are only used in the first step of the analyzer chain. They’re then immediately turned into a String based representation that can be &lt;code class=&quot;highlighter-rouge&quot;&gt;reset()&lt;/code&gt; properly. And, as an optimization, Tokenizer closes and throws away the underlying Reader in order to save memory.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;  &lt;span class=&quot;cm&quot;&gt;/**
   * {@inheritDoc}
   * &amp;lt;p&amp;gt;
   * &amp;lt;b&amp;gt;NOTE:&amp;lt;/b&amp;gt;
   * The default implementation closes the input Reader, so
   * be sure to call &amp;lt;code&amp;gt;super.close()&amp;lt;/code&amp;gt; when overriding this method.
   */&lt;/span&gt;
  &lt;span class=&quot;nd&quot;&gt;@Override&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;throws&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IOException&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// LUCENE-2387: don't hold onto Reader after close, so&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// GC can reclaim&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;inputPending&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ILLEGAL_STATE_READER&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;Tokenizer.java - &lt;a href=&quot;https://github.com/apache/lucene-solr/blob/branch_7_6/lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java&quot;&gt;Solr
7.6 Github&lt;/a&gt;&lt;/div&gt;

&lt;p&gt;It also happens that the current Tokenizer implementation doesn’t allow overriding in a way that allows subclasses to sidestep the problem if there’s a need to use them in another context. What that means for API users is that you cannot use Tokenizers in any place other than the first step of the analyzer chain, and you cannot use a Tokenizer to construct a ConcatenatingTokenStream.&lt;/p&gt;

&lt;p&gt;I personally feel this speaks to a deeper API problem. On the one hand, I understand throwing away the Reader. Readers themselves sometimes can’t be reset, so it’d be pointless to hold onto them after the initial use. On the other hand, TokenStream specifically states that subclasses must be able to be &lt;code class=&quot;highlighter-rouge&quot;&gt;reset()&lt;/code&gt;. Tokenizer’s inability to do that feels broken and is confusing for the API user. Perhaps Tokenizers should clean up their Reader but store the underlying tokens or strings to be used during the next lifecycle? After all, if the consumer isn’t going to process the stream again, then it should throw away the reference so the GC can clean up the whole stream. This would allow all TokenStreams to follow the same lifecycle and clear up the confusion around the contract.&lt;/p&gt;

&lt;p&gt;Enough of my opinion though, the point of this post is to help you understand how to use the API in your own projects. So, you’re writing a new TokenFilter/TokenFilterFactory, you need to be able to create new a TokenStream on-the-fly, and it needs to be constructed from some kind of String based source. In today’s API (Solr 7.6) your only choice is to use a Tokenizer. There is no public TokenStream that can be constructed from a String in the current API. But, Tokenizer doesn’t work in this situation. In my project, I had to build a new TokenStream that takes a String as input. Solr 8 will likely contain a new TokenStream class that does take a String as input.&lt;/p&gt;

&lt;p&gt;The main thing to beware of is that in the current implementation of the Lucene-Solr API (7.6), you can’t feed a Tokenizer to just any interface that takes a TokenStream, because somewhere down the line a consumer may try to &lt;code class=&quot;highlighter-rouge&quot;&gt;reset()&lt;/code&gt; it and Tokenizer can’t be reset.&lt;/p&gt;

&lt;h3 id=&quot;concatenating-token-stream&quot;&gt;Concatenating Token Stream&lt;/h3&gt;
&lt;p&gt;As I previously mentioned, I’d been working with the new ConcatenatingTokenStream class in my project. The second pitfall I came across is that ConcatenatingTokenStream is broken in Solr 7.6. It doesn’t restore its offsets when &lt;code class=&quot;highlighter-rouge&quot;&gt;end()&lt;/code&gt; is called, and it doesn’t &lt;code class=&quot;highlighter-rouge&quot;&gt;reset()&lt;/code&gt; properly. If you try to use it in Solr 7.6 (or earlier), you will very likely run into problems. Also, the problems can be hard to spot. In some cases, there are no errors thrown. Instead, your indexed content just becomes unsearchable. In other cases, specifically when you’ve got a copyField in use, you’ll run into an IllegalArgumentException talking about startOffset and endOffset being wrong. I did submit a &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-8650&quot;&gt;patch&lt;/a&gt; to fix the problems which was merged into Solr 7.7.&lt;/p&gt;

&lt;h3 id=&quot;happy-coding&quot;&gt;Happy Coding&lt;/h3&gt;
&lt;p&gt;I hope this post gives those of you wanting to use the TokenStream api a bit of a better understanding of how it all comes together, and helps you sidestep some potholes. Comments and feedback are always welcome.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>dan</name>
        
        
      </author>

      

      
        <category term="Lucene" />
      
        <category term="Solr" />
      
        <category term="search" />
      
        <category term="tokenstream" />
      
        <category term="tokenizer" />
      
        <category term="tokenfilter" />
      

      
        <summary type="html">I was recently tasked with a project for a client involving Lucene / Solr tokenizers and token streams. It’s common for most Solr users to only have to deal with tokenizers at a high level, usually when they’re building out their schema and deciding how their searchable content needs to be broken up and filtered. For example, adding a standard or whitespace tokenizer, a synonym filter, and a stemmer to the schema. However, the details of this particular project required a deeper level of involvement. During my investigation on how these things work at a lower level, I found there was very little documentation available. In fact, the only documentation I was able to find aside from the javadocs was a blog post by Michael McCandless that is almost seven years old! For those people wanting to use the API at a lower level, perhaps to write your own TokenFilter or do some custom tokenization, this isn’t much to go on, so I thought I’d do a bit of an update as well as write about some stumbling blocks I came across. But first, let’s do a quick API introduction.</summary>
      

      
      
    </entry>
  
</feed>
